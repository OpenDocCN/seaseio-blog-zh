# 在线测试在学习排名中的重要性——第一部分

> 原文：<https://web.archive.org/web/sease.io/2020/04/the-importance-of-online-testing-in-learning-to-rank-part-1.html>

您刚刚训练了一个学习排序模型，现在想知道它的表现如何。

你可以从在测试集上查看训练返回的评估参数开始，但你仍然不确定在真实网站中使用它会产生什么影响。

这就是在线测试的用武之地。它仍然是检查您的模型在真实场景中的表现的最佳方式，并且它可以为您提供必要的信息来评估、改进和更好地理解您的模型的行为。

在这篇博文中，我向你展示了学习排名的在线测试。
**我将讲述这种方法的优势以及为什么要使用它，最后我将向您展示如何实现它，以及在实现过程中要避免哪些错误。**

在第一部分中，我讨论了当前的技术水平，而在第二部分中，我将向您展示如何实现交错。

## 行业中

不幸的是，今天仍然有许多行业不使用在线测试来学习排名评估。也许他们不知道它的存在，或者他们没有合适的工具去做；他们可能没有理解结果的能力。
无论如何，这是一个遗憾，因为在线测试对于模型的正确调整和改进至关重要。

我们当然可以依靠离线评估，在测试集上检查模型的性能，或者通过分析不同的离线指标，但我们不能真正确定对网站的影响。

使用**离线评估**很难发现几个问题。

###### 使用错误的测试集，可能会获得不反映真实模型改进的模型评估结果

首先，我们可能没有正确地训练和/或测试模型。如果我们在创建训练和/或测试集时出错，我们将得到不可靠的结果，这些结果可能会被误解。使用错误的测试集，可能会获得更好的模型结果，而这些结果并没有反映真实的模型改进。在这种情况下，我们有一个使用离线评估无法捕捉的问题。

###### 什么时候测试集是错误的？

当它不能很好地代表和概括排名问题时。
一些例子:

*   *   ***每个查询组一个样本***:对一个查询的单个结果进行排序根本不是排序问题。
    *   ***一个相关性标签**用于一个查询组的所有样本*:所以无论模型做什么排序，离线评价指标都是一样的。
    *   *从**数据集*** 开始:你考虑哪些交互用于训练，哪些用于测试？训练集和测试集的划分是一个非常重要的阶段，需要深入研究。理想情况下，我们希望创建两个集合，它们在查询和相关性标签方面遵循相同的概率分布。

【T8

另一个问题是找到离线评估指标和我们为在线模型性能考虑的参数之间的直接关联。要了解线下指标的改善将如何反映在页面浏览量、点击量、销售额和收入上，这并不那么明显，也不简单。

###### 依赖生成的相关性标签可能是一个问题。这些并不总是可靠的，也不总是反映真实的用户需求

离线评估最常用的技术是**克兰菲尔德**技术。在这种方法中，一组专家从一组选定的文档开始创建一组相关性判断。特别是，它们为所选文档集中的每个文档分配一个相关性标签。这种范式的应用会带来两个主要问题:

*   *   关联集创建所需的 ***努力*** ，这是一个非常昂贵的阶段。
    *   依赖于生成的相关性标签。这些 ***并不总是可靠的*** 也不总是反映真实的用户需求。

【T29

如果我们想克服这些问题，我们必须通过**在线测试**。

## 在线测试

如前所述，使用在线测试有很多好处:

*   *   ***可靠性*** 的结果。在这种方法中，我们可以直接观察用户行为，并由此了解他最感兴趣的文档是什么。
    *   给出一个 ***直接解释*** 的结果。通过在线评估，我们可以直接看到该模型在页面浏览量、点击量、销售额或收入方面的结果。
    *   我们也可以更好地理解 ***模型行为*** 。我们可以看到用户如何与模型交互，并找出如何改进它。

很明显，这些好处来之不易，但它们是值得的。

当下，在学习排名及超越的世界里，业界广泛流传的在线测试有两种: **A/B 测试**和**交错**。

###### A/B 测试

让我们开始看看 A/B 测试是如何工作的。

既然我们在讨论学习排序，假设你有两个训练有素的学习排序模型:*模型 A* 和*模型 B* 。这些型号并不相同，您想要比较它们，以便找到最好的型号。

A/B 测试允许您进行这种比较。特别是，它允许您将查询流量分成两组。第一组用户使用*模式 A* 与网站互动，而第二组用户使用*模式 B* 与网站互动。通过这种方式，我们可以观察两种场景中的用户行为，并查看我们的目标指标，如点击量、销售额、收入，以了解哪种模式表现更好。

这是一个伟大的工具，它给了我们许多有用的信息，最重要的是，它依赖于真实的用户交互，应该反映真实的用户需求。

即使 A/B 测试很棒，我们也必须非常小心地实施它。

###### 不要做什么

假设我们有一个网站，其中我们使用我们的学习来为查询搜索页面排序模型。假设我们有 2 页:

1.  1.  *一个是首页*。这里我们展示几个有趣的可以直接购买的新文档。这些文档是静态的，并且**不是由学习排序模型**排序的。
    2.  *一个是搜索页面*。在这里，我们可以进行查询，并在结果中搜索我们最感兴趣的文档。这些文档实际上是由我们的学习排序模型进行排序的。

###### 当 A/B 测试时，我们需要确保我们只考虑来自**搜索结果页面的交互，这些页面由我们正在比较的**模型进行排名

测试时，我们必须确保我们考虑的所有结果都来自*搜索页面*，而不是来自*主页*。错误的分析确实会导致对模特表现的错误结论。

我要特别强调两种可能的情况:

*   *   我们得出的结论是:A 型*比 B 型*好，反之亦然。**
    *   我们正确地断定*A 型*优于*B 型*，但是我们无法确定改进的百分比。

先来看**第一个场景**。

*假设我们正在分析**模型 A** 。我们从主页获得 10 笔销售额，从搜索页面获得 5 笔销售额。*
*然后假设分析**模型 B** 。我们从主页获得 4 笔销售额，从搜索页面获得 10 笔销售额。*

如果我们只看总数，我们会看到:*A 型车*的 15 次销售和*B 型车*的 14 次销售，结论是*A 型车*是最好的。这对于主页(10 个销售> 4 个销售)是正确的，但是对于搜索页面(5 个销售< 10 个销售)却不是，在搜索页面中我们实际上使用了学习排名模型。

那么，我们该如何看待这一点呢？

这样，我们错误地断言*型号 A* 是最好的，而我们可以清楚地看到，在搜索页面*中，型号 B* 表现更好。
因此，我们不会在模型实际运行的页面上对模型进行评估，但是我们会根据不涉及模型的主页得出结论。

当改进只发生在主页而不发生在搜索页面时，同样的误解也会出现。下面你会看到一个例子。

*假设我们正在分析**模型 A** 。我们从主页获得 10 笔销售额，从搜索页面获得 10 笔销售额。*
*然后假设分析**模型 B** 。我们从主页获得 5 笔销售额，从搜索页面获得 10 笔销售额。*

在这里，如果我们只看总销售额，我们可以看到*型号 A* 获得 20 个销售额，而*型号 B* 获得 15 个销售额。在这种情况下，我们会得出结论:A 型车比 B 型车更好。

那么，我们该怎么做呢？

如果我们只看搜索页面的结果，这是不对的，因为两者都获得了相同的销售额(10)。
改进是由于主页没有使用模型，但我们可能会错误地将其归因于模型性能。

现在让我们看看第二个场景**。**

 ***假设我们正在分析**模型 A** 。我们从主页获得 12 笔销售额，从搜索页面获得 11 笔销售额。*
*然后假设分析**模型 B** 。我们从主页获得 5 笔销售额，从搜索页面获得 10 笔销售额。*

在这里，如果我们查看总销售额，我们可以看到*型号 A* 获得 23 笔销售额，而*型号 B* 获得 15 笔销售额。在这种情况下，我们会得出结论，A 型比 B 型更好。

那么，我们该怎么做呢？

这是真的，因为即使只看搜索页面，*A 型车*的销量也比*B 型车*的销量多(11 销量> 10 销量)。但是..改善了多少？从总数来看，我们看到*A 型*比*B 型*多获得了 8 的销量，这是一个巨大的进步！但实际上，仅查看搜索页面，我们就可以看到*A 型车*仅比*B 型车*多卖出 1 辆，差距并不大。**  **###### 在测试过程中过滤交互是很重要的。
我们必须只考虑与被评估模型真正相关的交互

这些例子表明，在测试过程中过滤我们的交互是很重要的。如果我们想要评估模型，我们必须只考虑与模型真正相关的交互。也考虑到其他人，像第一页的那些人，带来了额外的噪音，可以隐藏真正的模型性能。

###### 交叉

A/B 测试在工业中被广泛使用，它非常有用，并且实现起来并不复杂。
由于这些原因，这种方法已经成为大量研究的焦点，目的是改进当前的实施。
这些研究导致了一种新的类似方法，称为:**交错**。

在这种方法中，还比较了两个模型。与 A/B 测试的主要区别在于，在这里，我们直接比较两个模型，同时向同一用户显示它们的结果。
让我们更详细地解释它是如何工作的:

###### 交错可以防止用户在整个测试实验过程中接触到坏的模型

就 A/B 测试而言，交错有几个优点:

*   *   它减少了由于用户分组(A 组和 B 组)而导致的用户差异问题。
    *   在车型对比上更为敏感。
    *   它需要更少的交通。
    *   它需要更少的时间来获得可靠的结果。
    *   这并不一定会将一个糟糕的模型暴露给一部分用户

让我们看看交错是如何工作的，以及它是如何实现这些改进的。

###### 它是如何工作的

假设我们有两个学习排名模型叫做: *model_A* 和 *model_B* 。
给定一个查询 *q* 每个模型响应一个排序的文档列表:*l[A]和*l[B]。
此时，不是将列表*l[A]返回给一组用户 A，将列表*l[B]返回给一组用户 B，而是创建一个唯一的结果列表*l[I]并返回给用户。
查询流量不再分离，每个用户都可以看到使用相同交叉方法从两个模型中创建的结果列表。
因此，这个唯一列表将包含来自*型号 _A* 和*型号 _B* 的搜索结果。*****

有几种实现这种列表的方法，都尽量做到公平。

*我们对公平的理解是什么？* 这个概念与搜索结果的选择高度相关。
理想情况下，我们希望列表包含来自两个模型的相同数量的文档，因此一个模型相对于另一个模型的偏好不依赖于显示项目的数量。
其次，我们希望显示项目的位置不会影响用户对两种型号之一的偏好。如果我们将来自 *model_A* 的所有文档放在前 10 个位置上，并且将来自 *model_B* 的所有文档放在底层位置上，那么来自 B 的文档可能仅仅因为它们的低等级而没有被选择。

关于测试学习如何在行业中进行以及它在当前技术水平下看起来如何的第一次调查到此结束，但是如果你想了解如何实现这一点的更多细节，让我们在[第 2 部分](https://web.archive.org/web/20220930000022/https://sease.io/2020/05/online-testing-for-learning-to-rank-interleaving.html)见面吧！

// our service

## 不要脸的塞给我们培训和服务！

我有没有提到我们做[学习排名](https://web.archive.org/web/20220930000022/https://sease.io/learning-to-rank-training)和[搜索质量评估](https://web.archive.org/web/20220930000022/https://sease.io/training/search-quality-evaluation-trainings/search-quality-evaluation-training)培训？我们也提供这些主题的咨询，[如果你想让你的搜索引擎更上一层楼，请联系](https://web.archive.org/web/20220930000022/https://sease.io/contacts)！

// STAY ALWAYS UP TO DATE

## 订阅我们的时事通讯

你喜欢这篇关于学习排名在线测试的文章吗？不要忘记订阅我们的时事通讯，以便随时了解信息检索世界的最新动态！**